#+TITLE: LCATR Job Developer Instructions
#+SETUPFILE: setup.org

* Overview

This document describes how to develop job software while testing it under the Job Harness and what needs to be done in order to make releases of job software for production deployment.


** Requirements

The Job Harness runs your software through two main programs in order to assure required input files are available and to capture any produced output files.  It a production setting it also controls which versions of the job software are available and run. 

In order to provide this it make some requirements on the operating environment, job software and LIMS server.

*** Environment Requirements:

The running environment must provide:

 - access to the archive file server
 - a local staging disk area

*** Job Software Requirements

The job software must be written to provide:

 - a "producer" and a "validator" main programs which require no input other than what the harness provides
 - a registration of these files following a particular convention (via an Environment Modules =modulefile=)

*** LIMS requirements

For a job to run under the Job Harness it must first be registered with LIMS.  Only job name/version pairs that are registered may be run through the Job Harness.  It is also through LIMS that any dependencies are expressed.  If JobB needs the output file from JobA, it is through this registration where that is declared.


* Development

This section describes how to configure your account to develop your job software while running it under the Job Harness.  When a concrete example is shown it assumes the account name is *operator*, the job name is *analyze* and the job version is *devel*.  In a production deployment the job version should indicate an actual version. (v1.2.3, etc).

There is much flexibility in how development is done while still using the job harness.  Depending on the number of jobs and their granularity and interrelatedness these following examples may be expanded on.  

** Top-level working directory

Pick some top-level working directory to work in:

#+BEGIN_EXAMPLE
$ mkdir /home/operator/lcatr
$ cd /home/operator/lcatr
#+END_EXAMPLE

For the following sections, all relative directories will be w.r.t. this one.

** Job Harness software

For development, a production deployment of the Job Harness should be used.  It is installed into a =virtualenv= directory.  To set up your shell environment it is suggested to run and configure a subshell.  This can be done by making two short shell scripts:

In your top-level working directory, copy this into =job-harness-shell=:

#+BEGIN_SRC sh
#!/bin/bash
mydir=$(dirname $(readlink -f $BASH_SOURCE))
ssh-agent /bin/bash --rcfile $mydir/job-harness.sh
#+END_SRC

And, copy this into =job-harness.sh=:

#+BEGIN_SRC sh
#!/bin/bash
source VIRTUALENV/bin/activate
export PS1="(jh)\u@\h:\w> "
export https_proxy=http://lsstproxy.rcf.bnl.gov:3128/
export http_proxy=http://lsstproxy.rcf.bnl.gov:3128/
#+END_SRC

Now edit this last one:

#+BEGIN_EXAMPLE
$ emacs job-harness.sh
$ chmod +x job-harness-shell
#+END_EXAMPLE

And replace =VIRTUALENV= with the directory holding the =virtualenv= area.

You can now start your shell:

#+BEGIN_EXAMPLE
$ ./job-harness-shell
(jh)$ lcatr-harness --help
(jh)$ exit
#+END_EXAMPLE

On RACF one can start the job harness shell like:

#+BEGIN_EXAMPLE
~bvastro/lcatr/job-harness-shell
#+END_EXAMPLE

After running =job-harness-shell= you may notice your prompt now has a =(jh)= prepended to indicate a sub-shell was started.  At this point you can run the Job Harness with the =--help= flag to see its help page.  When done using the harness you can =exit= the shell and drop back to your normal one.

** The archive key

Your account needs an SSH private key that can be used to access the archive account.  Normally this is installed by the deployment system.  You may get one for use in development.

#+BEGIN_EXAMPLE
$ cp .... ~/.ssh/id_archive
$ chmod 400 ~/.ssh/id_archive
#+END_EXAMPLE

** Job Harness default configuration

The Job Harness takes many parameters.  To avoid having to specify them on the command line all the time it is recommended that they be placed in a configuration file which goes in the operators home directory.

Copy this to =~/.lcatr.cfg=:
#+BEGIN_EXAMPLE
[DEFAULT]
archive_host = astroftp.rhic.bnl.gov
lims_url = https://www.lsst.bnl.gov:8088
archive_user = ccdarxiv
unit_type = CCD
user = ccdtest
archive_root = /astro/astronfs01/ccdarchive/tst
stage_root = /PATH/TO/LOCAL/stage
#+END_EXAMPLE

Now, edit the file and set the =stage_root= variable to point to a
large disk which you can write to.

** The =modulefile=

Your job software main programs are located by the Job Harness with an [[http://modules.sf.net/][Environment Modules]] =modulefile=.  This file itself is located by searching for it in a sub-directory named like =<job name>/<job version>/modulefile=.  Make this directory in your working area

#+BEGIN_EXAMPLE
$ mkdir -p lcatr/analyze/devel
#+END_EXAMPLE

Or, if you already have code in a repository clone it into a "devel" target

#+BEGIN_EXAMPLE
$ mkdir -p lcatr/analyze/
$ git clone http://git.server//analyze lcatr/analyze/devel
#+END_EXAMPLE

Now, edit the =lcatr/analyze/devel/modulefile= to look something like:

#+BEGIN_EXAMPLE
#%Module1.0 #-*-tcl-*-#
source "$::env(LCATR_MODULES)/lcatr.tcl"
lcatr_package /home/operator/lcatr/analyze/devel/producer /home/operator/lcatr/analyze/devel/validator
#+END_EXAMPLE

This last line is the important one and sets the path to your =producer= and =validator= programs.  They should be adjusted to match their actual names.  Note, when readied for development this line should not include absolute paths to the producer/validator programs.

Note, you do not have to keep the producer/validator programs in the same directory as the =modulefile=.  You can also have multiple jobs implemented in one area or git repository.  However, each job needs a distinct =<job name>/<job version>/modulefile=.

** Register with LIMS

To register a job name/version pair with LIMS you must have a LIMS account.  At BNL you will likely have to have the special HTTP proxy set (the =job-harness-shell= example sets up the needed environment variables).  

#+BEGIN_EXAMPLE
https://www.lsst.bnl.gov:8088
#+END_EXAMPLE

Note that port 8088 is used.  This instance of LIMS is tied to the testing.  Do not use these developer instructions with the production LIMS!

** Run your job

Once the above has been set up you should be able to run your job software's =producer= and =validator= programs under the Job Harness.  

#+BEGIN_EXAMPLE
(jh)$ lcatr-harness --job analyze \
                    --version devel \
                    --unit-id 112-04 \
                    --operator bv \
                    --modules-path=/home/operator/lcatr
#+END_EXAMPLE

Any and all of these options may be placed in the =~/.lcatr.cfg= file.  The command line will override them.



* Use Python Packages

When your test station or analysis job is written in Python it is best to put virtually all of your code inside a Python /package/.  This will best assure that it runs the same in your development and production environments and minimize the amount of external environment that needs to be correct.

Here, the term Python /package/ means something specific in Python (and will be italicized here) and should not be confused with the more general usage in the context of the greater Job Harness system.  

The examples continue the assumption that your job is named "analyze" and all examples assume you have =cd='ed into your code directory (which was =analyze/devel/= in the previous section).  It is also assumed that you are working in the sub-shell started by running =job-harness-shell=.

** Make your Python /package/ directory

A Python /package/ is really nothing more than a directory with an =__init__.py= file (which may even be empty) marking the directory as such.  There is flexibility in where you make this directory.  During development we will let Python locate it by adjusting the =PYTHONPATH= environment variable.  In a production context Python will locate it because it will get installed in Python's =site-packages= system area.

Make the (initially) empty =analyze= /package/:

#+BEGIN_SRC sh :results silent :exports code
mkdir analyze
touch analyze/__init__.py
#+END_SRC

Let Python find this /package/ in your development environment.  You should still be in the directory holding your source (eg. =analyze/devel/=) when you issue this command as the output of =pwd= is used to set the directory:

#+BEGIN_SRC sh :eval no
export PYTHONPATH=`pwd`
#+END_SRC

Note, the =pwd= command is surrounded by backtick quotes.

You can now test that this Python package is correct right from the shell command line:

#+BEGIN_SRC sh :results output replace :exports both
python -c 'import analyze'
echo $?
#+END_SRC

#+RESULTS:
: 0

You should get no errors

** Adding modules to your Python /package/

Now you are all ready to add code into your Python /package/ directory.  Every file you make under =analyze/= will result in a Python module that can be accessed in Python code after the Python =import= operator is used.  Here we make a trivial example module by just =echo='ing a line of Python code into a file from the shell:

#+BEGIN_SRC sh :results silent
echo 'print "Hello World"' > analyze/hello.py
#+END_SRC

The =analyze/hello.py= file has just this one line, you would of course use an editor to place something more substantial:

#+BEGIN_SRC python :eval no 
print "Hello World"
#+END_SRC

Here is how you would use the module from some other Python code

#+BEGIN_SRC python :results output :exports both
import analyze.hello
#+END_SRC

#+RESULTS:
: Hello World

The second line shows the result you should see when you execute the Python code. 

Normally you would not place top-level code in a Python module file.  Here is a second example where a function is defined:

#+BEGIN_SRC sh :results none
echo 'def say(): print "Bye"' > analyze/goodbye.py
#+END_SRC

Then it may be used like:

#+BEGIN_SRC python :results output
  from analyze import goodbye
  def myfun():
      goodbye.say()
  myfun()
#+END_SRC

#+RESULTS:
: Bye

Or, you can import individual functions or classes from their module like:

#+BEGIN_SRC python :results output
  from analyze.goodbye import say
  say()
#+END_SRC

#+RESULTS:
: Bye

Continue to build up your Python /package/ by adding code or more Python files to the =analyze/= directory.  Sub-modules, for example =analyze.tools= can also be made:

#+BEGIN_SRC sh :results output replace :exports both
mkdir analyze/tools
touch analyze/tools/__init__.py
echo 'print "I am a tool sub-module"' > analyze/tools/sometool.py
python -c 'from analyze.tools import sometool'
echo
echo 'The Python package layout is now:'
echo
ls -R analyze
#+END_SRC

#+RESULTS:
#+begin_example
I am a tool sub-module

The Python package layout is now:

analyze:
goodbye.py
goodbye.pyc
hello.py
hello.pyc
__init__.py
__init__.pyc
tools

analyze/tools:
__init__.py
__init__.pyc
sometool.py
sometool.pyc
#+end_example

** Main producer/validator scripts

With a well organized Python /package/ your main /producer/ and /validator/ scripts that the Job Harness calls may become very trivial.  For example the producer might be just:

#+BEGIN_SRC python :eval no 
from analyze import producer
producer.run()
#+END_SRC


* Introduction to the rest

The following is mostly from an older version of this document but still applicable.

In order to be "harnessed" your software must be augmented to satisfy
the following aspects:

 - Provide an installation mechanism that fits into the versioning
   system.

 - Provide the required executable entry points.

 - Adhere to output file location rules.

It is expected that these aspects can be satisfied by writing thing
"shim" scripts that "glue" your existing software into the job
harness.  The expected, general steps to follow are:

 0) Develop your software.  This can be done separately from the rest
    of the steps but they are best taken under consideration.

 1) Develop the required two main programs (/producer/ and
    /validator/) called by the harness for each atomic job.

 2) Provide an environment /modulefile/ that describes the required
    environment variables and a suitable installation mechanism.

 3) Push tagged releases of your software into a remote git
    repository.

The instructions for each step are described in detail in the rest of
this document.  In addition, for information about testing your
software under the job harness see the document "[[./testing.org][LCATR Testing]]".

** Scope considerations

The scope of a harnessed job is a balance between being as inclusive
as possible to avoid a proliferation of jobs (and their developer
overheard) and being as atomic as possible so that one problem does
lead to an abort an loss of progress.  It's up to the developer to
determine this.  Some guidelines:

 - A job is considered successful for failed; there is no middle
   ground.  If it is to be re-run it must start from the beginning.

 - Success should be judged on the job finishing the processing with
   no fatal errors and not on the actual results (eg, a busted CCD can
   still be successfully tested).

 - The job must finish completely and successfully before its summary
   results are uploaded to LIMS.

 - Mechanisms for chaining separate but related jobs are possible.

** Naming

The harness identifies a job by its /canonical name/ (and its
installed /version/).  Once a name is selected it becomes a reserved
identifier in various parts of the system.  Since it is a primary
descriptor it is best for the named test to not evolve far beyond its
original intention.  If a radical change is needed then a new
canonical name should be chosen.  


* Develop your software

Your software may be written in a variety of ways but some general
rules to keep in mind:

 - it must run in "batch" or command line mode and not require any
   interactive input.  

 - it must write any important output files to the *current working
   directory*, writing outside of this directory is not allowed.

 - if it requires a MS Windows host it must be able to run from the
   Cygwin environment

It is recommended that output file names be static from job to job and
not encode timestamps, CCD IDs, version strings, etc as these are
reflected by the job harness in the output directory path and are
recorded to the LIMS database.  Encoding this information makes it
more difficult for downstream processing to locate input files.


* Two main programs

For each atomic job, the harness will run two programs one after the
other which must be provided by your software.  They may be named as
you like but first is identified as the /producer/ and the second as
the /validator/.  They will likely be written as interpreted scripts
but need not be.

The /producer/ program is meant to perform the primary functionality
of your test or analysis software.  It is free to produce any needed
output files which may be in any desired format.

The validator program has two responsibilities.  First, it should
perform any validation on the output of the producer that the author
sees fitting.  Second it must produce a =summary.lims= file that is in
a schema-compliant format.  See the document "[[./schema.org][LCATR Schema]]" for
details on producing this file.

* Environment

To define the environment your software must provide an Environment
Module[fn:modules].  In this file all variables required
by the harness to run your software must be defined.  These variables
can be defined automatically by including this boilerplate:

[fn:modules] http://modules.sf.net/

#+begin_src Tcl
#%Module1.0 #-*-tcl-*-#
source "$::env(LCATR_MODULES)/lcatr.tcl"
lcatr_package PRODUCER VALIDATOR
#+end_src

Replace the strings =PRODUCER= and =VALIDATOR= with the names of your
producer and validator executable files.  They should be specified as
paths relative to the software's top-level installation directory
(more on installation below).

If your software requires additional environment variables they may be
included in this file as well.  Keep in mind that any information
about individual units of testing have no place here.

Environment Modules are described at http://modules.sf.net/ and see
the document "[[./modulefiles.org][LCATR Environment Modules]]" for details about writing
them for your software.

** Finding input files

One special environment variable that your producer will need to use
if it reads in files that were produced from other jobs that have been
run on the same unit of test is the =LCATR_DEPENDENCY_PATH=.  This
variable holds a ":"-delimited list of directories that contain output
file from the prior jobs run on the same CCD (or RTM) that your
software is testing.  This directory path list is the sole mechanism
for dependent packages to find input files that were produced as
output from prior jobs.  

If you write your producer in Python the function =dependency_glob()=
will help you search for a files matching a pattern in this path.

#+BEGIN_SRC python
  from lcatr.harness.helpers import dependency_glob
  filenames = dependency_glob("somefile_*.data")
  for filename in filenames:
      f = open(filenames[0])
      data = f.read()
      # ...
#+END_SRC

** Finding the CCD ID

You tell the job harness which CCD ID is being run with the =--unit-id= command line flag (or equivalent configuration file variable) and it then tells your job via the =LCATR_UNIT_ID= environment variable.  Here is how your Python code can get this information:

#+BEGIN_SRC python
  import os
  ccd_id = os.environ['LCATR_UNIT_ID']
#+END_SRC

* Tagged release

All production code must be specified by a tag in a git repository.
This section gives brief commands on how to perform the required git
mechanics.

** Git repository setup

Before getting starting a repository must be requested from the site
git repository manager.  You should provide the canonical name for
your software and you will be given a git URL.  If you do not yet have
a local git repository you will clone the (empty) new one and fill it
with your software files.  If you are already using git to manage your
software you will add this new repository as a remote.  The following
examples use an existing (non-empty) repository.

Clone a remote git repository:

#+begin_example
git clone https://git.racf.bnl.gov/astro/git/lcatr/jobs/example_station_A.git
#+end_example

When done, the remote name defaults to "origin".  To add a remote
repository to an already existing local one do:

#+begin_example
git remote add <remotename> https://git.racf.bnl.gov/astro/git/lcatr/jobs/example_station_A.git
#+end_example

** Tagging a release

To tag a release one runs the following from your local git-controlled area

#+begin_example
git tag <tagname> [<commit> | <object>]
#+end_example

The form of the tag name is left to your discretion but it is
recommended that some convention is chosen and carefully followed.  By
default this will tag the current "head" commit.  If another commit is
desired its hash may be specified.  Running =gitk= can help identify
the commit.

Git allows for changing a tag after it has been made.  This should be
strongly avoided but, strictly speaking, it will only cause a problem
if done after the tag was used to install, select and run your
software in a production setting.

** Pushing a release

So far you have only made changes to your local repository.  To send
the commits associated with the tag you "push" them like:

#+begin_example
git push <remotename> <tagname>
#+end_example

** Advertising a release

After the git mechanics above are complete, the next step is to inform
the software manager that a new release has been made and requires
installation.  This notification should include at least:

 - the tag name you chose to release

 - the remote (cgit) repository URL

 - on which machines the software needs installing

 - installation and validation instructions

It is expected that only the first item will change from release to
release.




* Dependencies

As described above, your job finds input files that were output by any
jobs on which your job depends based on information that the harness
provides.  The harness gets this information from the LIMS database
but it must ultimately come from the test software developer.  You
need to communicate this to LIMS.

* Installation

Specifics are found in the document [[./installation.org][LCATR Job Installation]].


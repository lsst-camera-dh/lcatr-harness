#+TITLE: LCATR Job Developer Instructions
#+SETUPFILE: setup.org

* Overview

This document describes how to develop job software while testing it under the Job Harness and what needs to be done in order to make releases of job software for production deployment.


** Requirements

The Job Harness runs your software through two main programs in order to assure required input files are available and to capture any produced output files.  It a production setting it also controls which versions of the job software are available and run. 

In order to provide this it make some requirements on the operating environment, job software and LIMS server.

*** Environment Requirements:

The running environment must provide:

 - access to the archive file server
 - a local staging disk area

*** Job Software Requirements

The job software must be written to provide:

 - a "producer" and a "validator" main programs which require no input other than what the harness provides
 - a registration of these files following a particular convention (via an Environment Modules =modulefile=)

*** LIMS requirements

For a job to run under the Job Harness it must first be registered with LIMS.  Only job name/version pairs that are registered may be run through the Job Harness.  It is also through LIMS that any dependencies are expressed.  If JobB needs the output file from JobA, it is through this registration where that is declared.


* Development

This section describes how to configure your account to develop your job software while running it under the Job Harness.  When a concrete example is shown it assumes the account name is *operator*, the job name is *analyze* and the job version is *devel*.  In a production deployment the job version should indicate an actual version. (v1.2.3, etc).

There is much flexibility in how development is done while still using the job harness.  Depending on the number of jobs and their granularity and interrelatedness these following examples may be expanded on.  

** Top-level working directory

Pick some top-level working directory to work in:

#+BEGIN_EXAMPLE
$ mkdir /home/operator/lcatr
$ cd /home/operator/lcatr
#+END_EXAMPLE

For the following sections, all relative directories will be w.r.t. this one.

** Job Harness software

For development, a production deployment of the Job Harness should be used.  It is installed into a =virtualenv= directory.  To set up your shell environment it is suggested to run and configure a subshell.  This can be done by making two short shell scripts:

In your top-level working directory, copy this into =job-harness-shell=:

#+BEGIN_SRC sh
#!/bin/bash
mydir=$(dirname $(readlink -f $BASH_SOURCE))
ssh-agent /bin/bash --rcfile $mydir/job-harness.sh
#+END_SRC

And, copy this into =job-harness.sh=:

#+BEGIN_SRC sh
#!/bin/bash
source VIRTUALENV/bin/activate
export PS1="(jh)\u@\h:\w> "
export https_proxy=http://lsstproxy.rcf.bnl.gov:3128/
export http_proxy=http://lsstproxy.rcf.bnl.gov:3128/
#+END_SRC

Now edit this last one:

#+BEGIN_EXAMPLE
$ emacs job-harness.sh
$ chmod +x job-harness-shell
#+END_EXAMPLE

And replace =VIRTUALENV= with the directory holding the =virtualenv= area.

You can now start your shell:

#+BEGIN_EXAMPLE
$ ./job-harness-shell
(jh)$ lcatr-harness --help
(jh)$ exit
#+END_EXAMPLE

After running =job-harness-shell= you may notice your prompt now has a =(jh)= prepended to indicate a sub-shell was started.  At this point you can run the Job Harness with the =--help= flag to see its help page.  When done using the harness you can =exit= the shell and drop back to your normal one.

** The archive key

Your account needs an SSH private key that can be used to access the archive account.  Normally this is installed by the deployment system.  You may get one for use in development.

#+BEGIN_EXAMPLE
$ cp .... ~/.ssh/id_archive
$ chmod 400 ~/.ssh/id_archive
#+END_EXAMPLE

** Job Harness default configuration

The Job Harness takes many parameters.  To avoid having to specify them on the command line all the time it is recommended that they be placed in a configuration file which goes in the operators home directory.

Copy this to =~/.lcatr.cfg=:
#+BEGIN_EXAMPLE
[DEFAULT]
archive_host = astroftp.rhic.bnl.gov
lims_url = https://www.lsst.bnl.gov:8088
archive_user = ccdarxiv
unit_type = CCD
user = ccdtest
archive_root = /astro/astronfs01/ccdarchive/tst
stage_root = /PATH/TO/LOCAL/stage
#+END_EXAMPLE

Now, edit the file and set the =stage_root= variable to point to a
large disk which you can write to.

** The =modulefile=

Your job software main programs are located by the Job Harness with an [[http://modules.sf.net/][Environment Modules]] =modulefile=.  This file itself is located by searching for it in a sub-directory named like =<job name>/<job version>/modulefile=.  Make this directory in your working area

#+BEGIN_EXAMPLE
$ mkdir -p lcatr/analyze/devel
#+END_EXAMPLE

Or, if you already have code in a repository clone it into a "devel" target

#+BEGIN_EXAMPLE
$ mkdir -p lcatr/analyze/
$ git clone http://git.server//analyze lcatr/analyze/devel
#+END_EXAMPLE

Now, edit the =lcatr/analyze/devel/modulefile= to look something like:

#+BEGIN_EXAMPLE
#%Module1.0 #-*-tcl-*-#
source "$::env(LCATR_MODULES)/lcatr.tcl"
lcatr_package /home/operator/lcatr/analyze/devel/producer /home/operator/lcatr/analyze/devel/validator
#+END_EXAMPLE

This last line is the important one and sets the path to your =producer= and =validator= programs.  They should be adjusted to match their actual names.  Note, when readied for development this line should not include absolute paths to the producer/validator programs.

Note, you do not have to keep the producer/validator programs in the same directory as the =modulefile=.  You can also have multiple jobs implemented in one area or git repository.  However, each job needs a distinct =<job name>/<job version>/modulefile=.

** Register with LIMS

To register a job name/version pair with LIMS you must have a LIMS account.  At BNL you will likely have to have the special HTTP proxy set (the =job-harness-shell= example sets up the needed environment variables).  

#+BEGIN_EXAMPLE
https://www.lsst.bnl.gov:8088
#+END_EXAMPLE

Note that port 8088 is used.  This instance of LIMS is tied to the testing.  Do not use these developer instructions with the production LIMS!

** Run your job

Once the above has been set up you should be able to run your job software's =producer= and =validator= programs under the Job Harness.  

#+BEGIN_EXAMPLE
(jh)$ lcatr-harness --job analyze \
                    --version devel \
                    --unit-id 112-04 \
                    --operator bv \
                    --modules-path=/home/operator/lcatr
#+END_EXAMPLE

Any and all of these options may be placed in the =~/.lcatr.cfg= file.  The command line will override them.



* Introduction to the rest

The following is mostly from an older version of this document but still applicable.

In order to be "harnessed" your software must be augmented to satisfy
the following aspects:

 - Provide an installation mechanism that fits into the versioning
   system.

 - Provide the required executable entry points.

 - Adhere to output file location rules.

It is expected that these aspects can be satisfied by writing thing
"shim" scripts that "glue" your existing software into the job
harness.  The expected, general steps to follow are:

 0) Develop your software.  This can be done separately from the rest
    of the steps but they are best taken under consideration.

 1) Develop the required two main programs (/producer/ and
    /validator/) called by the harness for each atomic job.

 2) Provide an environment /modulefile/ that describes the required
    environment variables and a suitable installation mechanism.

 3) Push tagged releases of your software into a remote git
    repository.

The instructions for each step are described in detail in the rest of
this document.  In addition, for information about testing your
software under the job harness see the document "[[./testing.org][LCATR Testing]]".

** Scope considerations

The scope of a harnessed job is a balance between being as inclusive
as possible to avoid a proliferation of jobs (and their developer
overheard) and being as atomic as possible so that one problem does
lead to an abort an loss of progress.  It's up to the developer to
determine this.  Some guidelines:

 - A job is considered successful for failed; there is no middle
   ground.  If it is to be re-run it must start from the beginning.

 - Success should be judged on the job finishing the processing with
   no fatal errors and not on the actual results (eg, a busted CCD can
   still be successfully tested).

 - The job must finish completely and successfully before its summary
   results are uploaded to LIMS.

 - Mechanisms for chaining separate but related jobs are possible.

** Naming

The harness identifies a job by its /canonical name/ (and its
installed /version/).  Once a name is selected it becomes a reserved
identifier in various parts of the system.  Since it is a primary
descriptor it is best for the named test to not evolve far beyond its
original intention.  If a radical change is needed then a new
canonical name should be chosen.  


* Develop your software

Your software may be written in a variety of ways but some general
rules to keep in mind:

 - it must run in "batch" or command line mode and not require any
   interactive input.  

 - it must write any important output files to the *current working
   directory*, writing outside of this directory is not allowed.

 - if it requires a MS Windows host it must be able to run from the
   Cygwin environment

It is recommended that output file names be static from job to job and
not encode timestamps, CCD IDs, version strings, etc as these are
reflected by the job harness in the output directory path and are
recorded to the LIMS database.  Encoding this information makes it
more difficult for downstream processing to locate input files.

* Two main programs

For each atomic job, the harness will run two programs one after the
other which must be provided by your software.  They may be named as
you like but first is identified as the /producer/ and the second as
the /validator/.  They will likely be written as interpreted scripts
but need not be.

The /producer/ program is meant to perform the primary functionality
of your test or analysis software.  It is free to produce any needed
output files which may be in any desired format.

The validator program has two responsibilities.  First, it should
perform any validation on the output of the producer that the author
sees fitting.  Second it must produce a =summary.lims= file that is in
a schema-compliant format.  See the document "[[./schema.org][LCATR Schema]]" for
details on producing this file.

* Environment

To define the environment your software must provide an Environment
Module[fn:modules].  In this file all variables required
by the harness to run your software must be defined.  These variables
can be defined automatically by including this boilerplate:

[fn:modules] http://modules.sf.net/

#+begin_src Tcl
#%Module1.0 #-*-tcl-*-#
source "$::env(LCATR_MODULES)/lcatr.tcl"
lcatr_package PRODUCER VALIDATOR
#+end_src

Replace the strings =PRODUCER= and =VALIDATOR= with the names of your
producer and validator executable files.  They should be specified as
paths relative to the software's top-level installation directory
(more on installation below).

If your software requires additional environment variables they may be
included in this file as well.  Keep in mind that any information
about individual units of testing have no place here.

Environment Modules are described at http://modules.sf.net/ and see
the document "[[./modulefiles.org][LCATR Environment Modules]]" for details about writing
them for your software.

** Finding input files

One special environment variable that your producer will need to use
if it reads in files that were produced from other jobs that have been
run on the same unit of test is the =LCATR_DEPENDENCY_PATH=.  This
variable holds a ":"-delimited list of directories that contain output
file from the prior jobs run on the same CCD (or RTM) that your
software is testing.  This directory path list is the sole mechanism
for dependent packages to find input files that were produced as
output from prior jobs.  

If you write your producer in Python the function =dependency_glob()=
will help you search for a files matching a pattern in this path.

#+BEGIN_SRC Python
from lcatr.harness.helpers import dependency_glob
filenames = dependency_glob("somefile_*.data")
for filename in filenames:
  f = open(filenames[0])
  data = f.read()
  ...
#+END_SRC




* Tagged release

All production code must be specified by a tag in a git repository.
This section gives brief commands on how to perform the required git
mechanics.

** Git repository setup

Before getting starting a repository must be requested from the site
git repository manager.  You should provide the canonical name for
your software and you will be given a git URL.  If you do not yet have
a local git repository you will clone the (empty) new one and fill it
with your software files.  If you are already using git to manage your
software you will add this new repository as a remote.  The following
examples use an existing (non-empty) repository.

Clone a remote git repository:

#+begin_example
git clone https://git.racf.bnl.gov/astro/git/lcatr/jobs/example_station_A.git
#+end_example

When done, the remote name defaults to "origin".  To add a remote
repository to an already existing local one do:

#+begin_example
git remote add <remotename> https://git.racf.bnl.gov/astro/git/lcatr/jobs/example_station_A.git
#+end_example

** Tagging a release

To tag a release one runs the following from your local git-controlled area

#+begin_example
git tag <tagname> [<commit> | <object>]
#+end_example

The form of the tag name is left to your discretion but it is
recommended that some convention is chosen and carefully followed.  By
default this will tag the current "head" commit.  If another commit is
desired its hash may be specified.  Running =gitk= can help identify
the commit.

Git allows for changing a tag after it has been made.  This should be
strongly avoided but, strictly speaking, it will only cause a problem
if done after the tag was used to install, select and run your
software in a production setting.

** Pushing a release

So far you have only made changes to your local repository.  To send
the commits associated with the tag you "push" them like:

#+begin_example
git push <remotename> <tagname>
#+end_example

** Advertising a release

After the git mechanics above are complete, the next step is to inform
the software manager that a new release has been made and requires
installation.  This notification should include at least:

 - the tag name you chose to release

 - the remote (cgit) repository URL

 - on which machines the software needs installing

 - installation and validation instructions

It is expected that only the first item will change from release to
release.




* Dependencies

As described above, your job finds input files that were output by any
jobs on which your job depends based on information that the harness
provides.  The harness gets this information from the LIMS database
but it must ultimately come from the test software developer.  You
need to communicate this to LIMS.

* Installation

Specifics are found in the document [[./installation.org][LCATR Job Installation]].


#+TITLE: Design for LSST CCD/RTM Test Software System
#+AUTHOR: Brett Viren
#+EMAIL: bv@bnl.gov
#+DATE: \today

#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \hypersetup{
#+LATEX_HEADER:   hyperindex=true,
#+LATEX_HEADER:   plainpages=false,
#+LATEX_HEADER:   colorlinks=true,
#+LATEX_HEADER:   linkcolor=black
#+LATEX_HEADER: }

#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:
#+XSLT:

The individual LSST camera parts (CCDs and their larger aggregates in
the form of RTMs) undergo a series of acceptance tests and analysis.
Many of the procedures for this are implemented by running software of
some sort.  This software runs on a variety of platforms, is
programmed in a variety of languages and is written by collaborators
and some may be commercial/proprietary.
Each software-based procedure is broken down into one or
more /jobs/.  The running of these jobs and the organization of their
results are managed by higher level software dubbed the /job harness/.
Its primary purpose is to manage the complexity from the diversity of software.
The harness also interfaces the jobs with the Laboratory Information
Management System (LIMS) and with the /archive/ file system.  This note
describes the design of the job harness, its interaction with LIMS and
the requirements that it makes on the software that it runs.
(This note is version src_sh[:exports results]{git show-ref -s refs/heads/master}.)


* Names and Terms

Some elements of the system are given canonical identifiers.  These
names are used, following a shared convention, by the job harness,
LIMS among other systems.  Things with canonical identifiers include:

 - unit type :: type of part, eg "CCD" or "RTM" 

 - unit id :: a brief but globally unique identifier for the unit (eg,
              the BNL CCD ID)

 - job name :: the name the software which implements one unit of
               processing (eg "station3" or "ptc")

 - job version :: a brief label identifying the version of the
                  software comprising the job's code.  It must match
                  an associated Git commit tag of the software that
                  the job runs.

 - site name :: the location running the tests (including those hosting development testing)

 - output directory :: a canonical pattern made up of other
      identifiers what describe the run of a job.

Besides these there are additional terms that are used with specific
meaning in this note.

 - test :: production of data from the examination of a unit or from
           analyzing output from another test.  

 - station :: a physical test device that generates data by taking
              measurements of a unit either by manual, human effort or by
              invoking a software process (ie, a job).

 - analysis :: a pure software test operating on the output files of
               another test and producing derived output files of its
               own.  Made up of one or more production jobs.

 - job :: running one instance of a specific installed version of test
          software on one unit (CCD/RTM).

 - software :: the software provided by a test developer that implements a job.

 - program :: an executable file (producer or validator, see below)
              provided by an installed version of the software which
              is called in the course of a job.

 - archive :: the root of a site-central, organized file-system
              hierarchy storing all result files of all jobs.

 - stage :: the root of a file-system local to the computer running a
            job.  It temporarily stores the job's output files and,
            where needed, holds a temporary copy of the output of
            prior jobs needed as input to the current job.

 - operator :: an identified individual charged with running a job
               via the harness.


* Cycle of Job Execution.

The role of the job harness is described briefly in this section by
breaking down the cycle of executing one job into a number of steps.
The following sections contain some additional details.

 - configuration :: the parameters to configure the job are determined
                    and communicated to the harness.  See the Section
                    [[Configuration]] for details.

 - job ID allocation :: a unique identifier for the job is allocated
      and any job dependencies are resolved.  See the Section 
      [[Job Identifier Allocation]] for details.

 - job environment :: the job software run-time environment is
      configured.  See the Section [[Job Environment]] for details.

 - staging :: local file system is prepared for the job.  See 
              Section [[File System]] for details.

 - running :: the software itself is run in two steps.  One producing
              the data and a follow up step that validates the data
              and prepares a summary in standard file formats.  See
              the Section [[Production and Validation Steps]] for
              details.

 - archiving :: the resulting files from the job are copied to the
                archive file system.  Again, see Section [[File System]] for details.

 - termination :: after an optional clean up of local files the
                  harness terminates.


* Configuration 

Each invocation of the harness is parameterized.  Parameters may be
set through a variety of means: environment, configuration files or
user interface (command line) arguments.

The configuration parameters are resolved in four ordered steps:

 1) The run-time environment may provide initial default parameters.

 2) Configuration files are checked and any parameters defined in
    a section named =[DEFAULT]= will update the configuration.

 3) User interface (eg command line) arguments are applied to the
    configuration.

 4) The configuration files are checked again for any sections that
    are named after an existing parameter name/value pair

Specifics of each configuration step are given in the following sections.


** Configuration Through the Environment

Configuration parameters may be specified with environment variables.
A parameter's variable name must be constructed by capitalizing the parameter
name and prepending it with =LCATR_=.  Some of these variables may be
specified in a /modulefile/ as described in Section [[Job Environment]].

** Configuration Files

Zero or more configuration files found in various locations will be
read by the harness.  The files checked are:

#+begin_example
~/.lcatr.cfg
./lcatr.cfg
#+end_example

They may provide configuration parameters in two ways.  First, the
=[DEFAULT]= section overrides any parameters supplied by environment
variables.  For example one could "hard code" the unit type for all
tests run from a given account by adding to =~/.lcatr.cfg= a section
like:

#+begin_example
[DEFAULT]
unit_type = CCD
#+end_example

This could then be overridden via parameters set by the user
interface.

Second, after any command line parameters are applied (see below) the
files are checked a second time for any sections that match the
key/value name of any existing parameters.  For example a section
named =[site BNL]= matches the parameter =site= with the value =BNL=
and might it define information about that site's archive and the
operator of the account in which the configuration file is placed.

#+begin_example
[site BNL]
archive_root = /lssd/nfsdata0/ccdtest
archive_user = bvastro
archive_host = rftpexp.rhic.bnl.gov
operator = bviren
#+end_example

Any parameters listed in the matching section and not yet set by
another mechanism will be applied but they will not override any
previously set parameters.  That is, this mechanism allows
specification of a set of default parameters that may be activated
based on the value of another parameter.  Setting any of these
parameters via another means will override their settings here.

** User Interface Parameters

The main Python =job.Job= class constructor accepts as keywords zero
or more parameters.  The default, command line interface to the
harness can likewise be given parameters via arguments like:

#+begin_example
--parameter_name=value
#+end_example

This will set the parameter =parameter_name= to the value =value=.
Setting parameters by via the user interface overrides any values that
may have been set by other means.

** List of Configuration Parameters

The configuration parameters understood by the harness are:

 - =context= :: a convenience identifier formed by a combination of
                =site=, =local= and =job=.  It may be used to organize
                default parameters.

 - =site= :: a (canonical) identifier for a site.

 - =local= :: a convenience identifier for the local computing
              environment (machine) running the harness.  It may be
              used to organize default parameters.

 - =job= :: the (canonical) name for a job.

 - =version= :: the job's version string matching a Git tag used to make a
                release and installation of the software for a job.

 - =operator= :: the username of the account invoking the harness

 - =archive_root= :: the absolute path to the root of the archive file
                     system.

 - =archive_user= :: the name of the user that has SSH access to the machine providing
                     =archive_root=.

 - =archive_host= :: the host name of the computer providing the
                     archive file system.

 - =stage_root= :: the absolute path to the root of the stage file
                   system local to the computer running the job.

 - =dependencies= :: a colon-separated list of job name/version pairs
                     on which the current job depends.  Note, this is
                     for testing the harness independently from LIMS.

 - =modules_home= :: a local directory containing the installation of
                     Modules (specifically containing the =init/= sub
                     directory)

 - =modules_version= :: the version of the Modules installation .

 - =modules_cmd= :: the path to the =modulescmd= program.

 - =modules_path= :: a colon-separated path of in which to search for
                     /modulefiles/.




* Job Environment 

Based on the input parameters and the allocated job ID the
job-specific environment is configured.  This environment is defined
through an Environment Module[fn:envmod] description file called a
/modulefile/.  Besides job-specific environment the /modulefile/ must
adhere to various conventions in support of the job harness.  The
software implementing a job must be installed on the host computer in
a manner that controls and records its version.  Details on this are
available elsewhere.


[fn:envmod] See http://modules.sf.net/.



* File System

All output files from production jobs are stored on disk in an
organized file system directory hierarchy.  The full set of result
files are centrally stored for each site in the /archive/ directory
hierarchy.  Each time a job runs a local /stage/ directory hierarchy
is first populated as needed.  If a job requires as input the files
output from a prior job they are copied to the local stage by the
harness.  When the job runs it populates its own specific directory
with any output files.  If the job is successful the harness copies
the freshly produced results into the corresponding directory in the
archive.

** File system layout

The archive and locally staged file hierarchy are organized in a
specific manner and based on the input parameters given to the job
harness and the allocated job ID.  Each run of each job results in a
unique directory being created and populated, first rooted in the local
stage and, if the job succeeds, copied to the central site archive.

The directory chain is named, in order, using the:

 - unit type (eg =CCD/=)
 - unit ID (eg =1234/=)
 - job name (eg =ptc/=)
 - job version (eg =v42/=)
 - job ID (eg =001234/=)

The pattern describing the directory layout is shared by the job
harness, the ingest process, the LIMS web application and any others
that must locate a file. 

** Staging and Archiving

Before a job's production step runs (see below) the local stage file
system is checked to assure the job's output directory does not yet
exist.  The archive is checked to assure the existence of a directory
associated with the run of each job on which the current job depends.
If these checks fail the harness aborts.  Otherwise the dependency
directories are copied from the archive to their corresponding
locations on the local stage and the job's output directory is created
and becomes the harness's current working directory.  Likewise, after
the validation step succeeds the job's directory and its contents are
copied from the local stage to the archive.  This checking and copying
are done over an SSH connection


* Production and Validation Steps

The running of the software is split into two steps.  A production step
produces files of results in whatever formats are convenient to the test
software developer.  A followup validation step provides a hook to
validate the content of these files and produce the required result
summary and meta-data files in the standard formats.


** Job Production Step

This step implements the main part of the job.  It involves running a
single program from the prepared and empty output directory as
described above, logging any output and checking the return code
(nonzero indicates failure).  This program will typically be
implemented as a thin script that glues into the job harness whatever
code the test owner has developed.  The program
to run is specified by the =LCATR_JOB_PRODUCER= environment variable
that is set in the /modulefile/ corresponding to the installed job
software release.  When executed by the harness, the program is given
no command line arguments.  Any input parameters it needs must be
taken from the environment.  This includes the standard =LCATR_=
variables defined through the configuration mechanism and any
software-specific variables defined in the /modulefile/.

With these requirements satisfied the program must otherwise fulfill
whatever duties it was written for and produce whatever result files
in whatever format the software developer has determined.


** Job Validation Step

From the point of view of the harness, this step is essentially the
same as the production step and only differs in what it is expected to
accomplish.  Again a single program, as specified by the
=LCATR_JOB_VALIDATOR= variable, is executed with no command line
arguments.  It is run from the same directory as was the producer and
any output it creates is logged.  It too must return a nonzero error
code to indicate failure.

In addition to performing whatever validation is required by the test
developer the validation program must produce a single meta-data file
(called =metadata.fits=) and one or more results summary files in
formats and schema specific to the job.  These two files will
eventually be loaded into the LIMS database by the LIMS ingest process
described elsewhere.  Details of these file formats, schema and
validations are in the note "[[./schema.org][LSST CCD Acceptance Testing File Schema]]".
The harness will run the validation routines associated with each
file's schema.

* LIMS

The Laboratory Information Management System (LIMS) is briefly
described in this section with an emphasis on how it interfaces with
the harness.  Additional details on LIMS can be found elsewhere.

In addition to its other duties not directly related to the job
harness, LIMS captures all meta data and a summary of result data
about all tests done on a CCD (or RTM) unit.  It provides a database
containing the test history, current status and the eventual
acceptance judgment of each unit based on the test results.  It is
fronted with a web application that allows browsing and querying of
the database.

The following sections describe how the job harness interfaces with
LIMS.

** Job Identifier Allocation

Every production run of a job is given a site-unique identifier (job
ID).  This job ID is allocated through LIMS via an HTTP query which
registers the following information:

 - site name :: canonical name of the testing site

 - unit type :: the type of unit being tested (CCD/RTM)

 - unit ID :: the identifier of the unit

 - job name :: canonical name of the job

 - job version :: the version of the software to be run

 - operator :: user name of the account running the harness

LIMS replies to the query with the job ID.  If the job requires data
produced by other, prior jobs the query will also return the
registered information about those jobs necessary to locate their
output files.


** Job Status Bookkeeping.

Among its other duties, LIMS records the status of jobs as reported to
it by the job harness.  This status progresses through a series of
states starting with the registration described above.  All subsequent
states are recorded by providing the allocated job ID and  state
identifier from the following list.

 - registered :: as above

 - configured :: the job environment has been configured

 - staged :: files from any prior dependencies have been copied to the
             local stage and the job's output directory is created.

 - produced :: the primary program of the job (see below) has run successfully

 - validated :: the secondary program validated the output and
                produced required result summary and meta data files
                (see below)

 - archived :: files are successfully copied to the archive

 - purged :: local stage area has been cleared (optional)

 - ingested :: results have been ingested to LIMS (outside the duty of the harness)

After the /archived/ (and optionally the /purged/) state has been
reached the job harness exits.


** Ingesting Results

After the /archived/ state has been recorded the results are
candidates for ingesting into LIMS.  The details of this process are
described elsewhere but the ingest process will use job ID and LIMS to
resolve the initial registration parameters so that the /metadata/
file the one or more /result summary/ files can be located, their
contents read and uploaded to the LIMS database.






